{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import diffusers\n",
    "import torch\n",
    "from diffusers import DiffusionPipeline\n",
    "from diffusers.pipelines.audioldm2.modeling_audioldm2 import AudioLDM2UNet2DConditionModel\n",
    "from src.hooked_model.hooked_model_audioldm2 import HookedAudioLDM2Model\n",
    "import numpy as np\n",
    "import src.hooked_model.scheduler\n",
    "from src.hooked_model.utils import get_timesteps\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use the hooked model interface with model from diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"cvssp/audioldm2-large\"\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    ").to(\"cuda\")\n",
    "model = AudioLDM2UNet2DConditionModel.from_pretrained(\n",
    "    model_name,\n",
    "    subfolder=\"unet\",\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    ").to(\"cuda\")\n",
    "scheduler = src.hooked_model.scheduler.DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.scheduler = scheduler\n",
    "pipe.unet = model\n",
    "hooked_model = HookedAudioLDM2Model(\n",
    "    model=model,\n",
    "    scheduler=scheduler,\n",
    "    encode_prompt=pipe.encode_prompt,\n",
    "    get_timesteps=get_timesteps,\n",
    "    pipeline=pipe,\n",
    "    vae=pipe.vae,\n",
    ")\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to provide:\n",
    "- denoiser model - either UNet or Transformer based, the assumption is that it should predict noise\n",
    "- scheduler - it has to have certain fields and implement scale_model_input() and step() methods\n",
    "- encode_prompt - function that encodes prompt into embeddings\n",
    "- get_timesteps - function that returns discrete timesteps for the diffusion process given the number of inference steps\n",
    "- vae - VAE model for latent space encoding/decoding, if latent space model is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ['Male man singing a song. The low quality recording features a ballad song that contains sustained strings, mellow piano melody and soft male vocal singing over it. It sounds sad and soulful, like something you would hear at Sunday services. Male man voice.']\n",
    "num_inference_steps = 100\n",
    "seed = 22\n",
    "num_waveforms_per_prompt = 1\n",
    "audio_length_in_s = 9\n",
    "guidance_scale = 5.0\n",
    "negative_prompt = [\"Low quality, average quality.\"]\n",
    "\n",
    "\n",
    "out = hooked_model(\n",
    "    prompt=prompt,\n",
    "    generator=torch.Generator(device=\"cuda\").manual_seed(seed),\n",
    "    num_inference_steps=num_inference_steps,\n",
    "    guidance_scale=guidance_scale,\n",
    "    negative_prompt=negative_prompt,\n",
    "    audio_length_in_s=audio_length_in_s,\n",
    "    num_waveforms_per_prompt=num_waveforms_per_prompt,\n",
    ")\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.scheduler = diffusers.DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "out2 = pipe(\n",
    "    prompt=prompt,\n",
    "    generator=torch.Generator(device=\"cuda\").manual_seed(seed),\n",
    "    num_inference_steps=num_inference_steps,\n",
    "    guidance_scale=guidance_scale,\n",
    "    negative_prompt=negative_prompt,\n",
    "    audio_length_in_s=audio_length_in_s,\n",
    "    num_waveforms_per_prompt=num_waveforms_per_prompt,\n",
    ")[0]\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(np.isclose(out[0], out2[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to gather activations at specific positions in the model\n",
    "\n",
    "All you need to do is to provide a list of positions you want to cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Male man singing a song. The low quality recording features a ballad song that contains sustained strings, mellow piano melody and soft male vocal singing over it. It sounds sad and soulful, like something you would hear at Sunday services. Male man voice.'\n",
    "num_inference_steps = 100\n",
    "seed = 22\n",
    "num_waveforms_per_prompt = 1\n",
    "audio_length_in_s = 9\n",
    "guidance_scale = 5.0\n",
    "negative_prompt = \"Low quality, average quality.\"\n",
    "\n",
    "out, cache_dict = hooked_model.run_with_cache(\n",
    "    prompt=prompt,\n",
    "    generator=torch.Generator(device=\"cuda\").manual_seed(seed),\n",
    "    num_inference_steps=num_inference_steps,\n",
    "    guidance_scale=guidance_scale,\n",
    "    negative_prompt=negative_prompt,\n",
    "    audio_length_in_s=audio_length_in_s,\n",
    "    num_waveforms_per_prompt=num_waveforms_per_prompt,\n",
    "    positions_to_cache=[\"up_blocks.1.attentions.5\", \"up_blocks.1.attentions.10\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cache_dict['output']['up_blocks.1.attentions.5'].shape)\n",
    "print(cache_dict['output']['up_blocks.1.attentions.10'].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
